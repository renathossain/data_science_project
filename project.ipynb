{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BZHvH-6y51J3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     /home/renath/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import matplotlib as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn as sk\n",
        "import nltk\n",
        "import random\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "# Download the VADER lexicon for sentiment analysis\n",
        "nltk.download('vader_lexicon')\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "data_path = \"text_comments.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6Yf7FU7U4BR"
      },
      "source": [
        "We first explore the data in `text_comments.csv` by printing some rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4BoA6m5s74aZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           id  score    link_id                author          subreddit  \\\n",
            "0  t1_ftjl56l      4  t3_gzv6so             mega_trex  BeautyGuruChatter   \n",
            "1  t1_ftjpxmc      6  t3_gzv6so             [deleted]  BeautyGuruChatter   \n",
            "2  t1_gzzxfyt     22  t3_nodb9e             divadream  BeautyGuruChatter   \n",
            "3  t1_gzzy7nc     92  t3_no6qaj  Ziegenkoennenfliegen  BeautyGuruChatter   \n",
            "4  t1_h00tpbp     82  t3_nolx7p       meowrottenralph  BeautyGuruChatter   \n",
            "5  t1_ftlamij      1  t3_h0an62       somethingelse19  BeautyGuruChatter   \n",
            "6  t1_h01dtz3     28  t3_noo5e0           sasukesbutt  BeautyGuruChatter   \n",
            "7  t1_h01fl3q      2  t3_nn2hz7             Mika_Kyle  BeautyGuruChatter   \n",
            "8  t1_ftll1qn      6  t3_h0dpxq             [deleted]  BeautyGuruChatter   \n",
            "9  t1_ftlsbtj      2  t3_h0an62            angelicad6  BeautyGuruChatter   \n",
            "\n",
            "                                                body  created_utc  \n",
            "0  Does anyone have a good cruelty free one? The ...   1591755558  \n",
            "1  (stares at my soft glam i've had for like 3 ye...   1591758382  \n",
            "2  When Jenâ€™s initial reactions came out to the s...   1622398357  \n",
            "3   I think you mean a \\n>Highschool *fucking* bully   1622398743  \n",
            "4  Ugh. I was honestly hoping that this brand wou...   1622414834  \n",
            "5  She's 35 in 2020\\n\\nhttps://jezebel.com/no-off...   1591801024  \n",
            "6  Is haus labs still around? Iâ€™m so out of the loop   1622426299  \n",
            "7  But I thought it's all because she is oppresse...   1622427319  \n",
            "8  This is such a mature and professional respons...   1591807162  \n",
            "9                                       Definitely ðŸ˜Š   1591810532  \n"
          ]
        }
      ],
      "source": [
        "# Create chunker\n",
        "chunk_size = 40000\n",
        "tf_chunks = pd.read_csv(data_path, chunksize=chunk_size)\n",
        "\n",
        "# Display the first 10 rows\n",
        "print(tf_chunks.get_chunk(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFML2jIWVCG4"
      },
      "source": [
        "We explore how many comments a subreddit could have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2F23Wi418AXn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MLBTheShow           13415\n",
              "BeautyGuruChatter     7638\n",
              "MensRights            6321\n",
              "TrueOffMyChest        4769\n",
              "SaltLakeCity          3560\n",
              "OurPresident          2761\n",
              "Cosmere               1536\n",
              "Name: subreddit, dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "first_chunk = next(tf_chunks)\n",
        "first_chunk[\"subreddit\"].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Nr6clkBXkI"
      },
      "source": [
        "Next, we introduce the sentiment analysis tool called VADER (Valence Aware Dictionary and sEntiment Reasoner). The sentiment scores are represented as a dictionary with the following keys:\n",
        "\n",
        "    'neg': Negative sentiment score (proportion of the text that is negative)\n",
        "    'neu': Neutral sentiment score (proportion of the text that is neutral)\n",
        "    'pos': Positive sentiment score (proportion of the text that is positive)\n",
        "    'compound': Compound sentiment score (a normalized, weighted composite score that represents the overall sentiment of the text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XFN4MVWz8iU4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment Scores: {'neg': 0.0, 'neu': 0.482, 'pos': 0.518, 'compound': 0.8649}\n"
          ]
        }
      ],
      "source": [
        "# Sample text (replace this with your own data)\n",
        "text = \"I really enjoyed working with my team. They are so helpful and supportive.\"\n",
        "\n",
        "# Initialize the sentiment analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Get the sentiment scores\n",
        "sentiment_scores = sid.polarity_scores(text)\n",
        "\n",
        "# Display the sentiment scores\n",
        "print(\"Sentiment Scores:\", sentiment_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtED7ajGVuhp"
      },
      "source": [
        "Now, we take a sample of the dataset 100 to 1. (Do not run this, we have made the sample already)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd9-5ME-_xR-"
      },
      "outputs": [],
      "source": [
        "# Specify the number of rows to read in each chunk\n",
        "chunk_size = 1000\n",
        "\n",
        "# Specify the number of rows to sample from each chunk\n",
        "sample_size = 10\n",
        "\n",
        "# Specify the path for the output CSV file\n",
        "output_csv_path = 'sample.csv'\n",
        "\n",
        "# Create a CSV writer object for the first chunk\n",
        "first_chunk = True\n",
        "\n",
        "# Create a CSV reader object\n",
        "csv_reader = pd.read_csv(data_path, chunksize=chunk_size, encoding='utf-8')\n",
        "\n",
        "# Iterate over each chunk, sample 10 rows, and append them to the output CSV file\n",
        "for i, chunk in enumerate(csv_reader):\n",
        "    try:\n",
        "      # Sample 10 rows from each chunk\n",
        "      sampled_chunk = chunk.sample(n=sample_size, random_state=42)  # Adjust random_state as needed\n",
        "\n",
        "      # Append the sampled chunk to the output CSV file\n",
        "      sampled_chunk.to_csv(output_csv_path, mode='a', index=False, header=first_chunk)\n",
        "\n",
        "      # Update the first_chunk flag after the first iteration\n",
        "      first_chunk = False\n",
        "    except:\n",
        "      print(f\"Error in chunk {i + 1}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mql5YFZjWHbe"
      },
      "source": [
        "We explore the sample dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nKRl34m2JLA6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>score</th>\n",
              "      <th>link_id</th>\n",
              "      <th>author</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>body</th>\n",
              "      <th>created_utc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>t1_fozlgkc</td>\n",
              "      <td>84.0</td>\n",
              "      <td>t3_ga8hsp</td>\n",
              "      <td>[deleted]</td>\n",
              "      <td>BeautyGuruChatter</td>\n",
              "      <td>I felt like the British guy actually asked him...</td>\n",
              "      <td>1.588193e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>t1_fw10hrc</td>\n",
              "      <td>2.0</td>\n",
              "      <td>t3_hf5j57</td>\n",
              "      <td>MGDlikethebeer</td>\n",
              "      <td>BeautyGuruChatter</td>\n",
              "      <td>Damn I missed it. I hope someone recorded it s...</td>\n",
              "      <td>1.593143e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>t1_gijx7tq</td>\n",
              "      <td>6.0</td>\n",
              "      <td>t3_ksvuzc</td>\n",
              "      <td>uselesssubject</td>\n",
              "      <td>BeautyGuruChatter</td>\n",
              "      <td>Hmmm I was more wondering if it was affiliated...</td>\n",
              "      <td>1.610123e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>t1_gi8bmyh</td>\n",
              "      <td>2.0</td>\n",
              "      <td>t3_kppctm</td>\n",
              "      <td>pitolaser</td>\n",
              "      <td>BeautyGuruChatter</td>\n",
              "      <td>Thanks! Will check them out.</td>\n",
              "      <td>1.609884e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>t1_fofd5q7</td>\n",
              "      <td>6.0</td>\n",
              "      <td>t3_g6oxmu</td>\n",
              "      <td>forgotmyfuckingname</td>\n",
              "      <td>BeautyGuruChatter</td>\n",
              "      <td>Same here. When I cook/bake recipes that take ...</td>\n",
              "      <td>1.587733e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>t1_fvzbhby</td>\n",
              "      <td>22.0</td>\n",
              "      <td>t3_hf5j57</td>\n",
              "      <td>severaldogs</td>\n",
              "      <td>BeautyGuruChatter</td>\n",
              "      <td>I honestly think itâ€™s why he uploaded the demo...</td>\n",
              "      <td>1.593110e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>t1_fvwg9vn</td>\n",
              "      <td>3.0</td>\n",
              "      <td>t3_hf5j57</td>\n",
              "      <td>hygsi</td>\n",
              "      <td>BeautyGuruChatter</td>\n",
              "      <td>I've noticed this too, there have been rumors ...</td>\n",
              "      <td>1.593039e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>t1_fvpa78v</td>\n",
              "      <td>64.0</td>\n",
              "      <td>t3_he33fn</td>\n",
              "      <td>OneHappyOne</td>\n",
              "      <td>BeautyGuruChatter</td>\n",
              "      <td>Yeah, I think that's what a lot of people get ...</td>\n",
              "      <td>1.592878e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>t1_fw5dw20</td>\n",
              "      <td>4.0</td>\n",
              "      <td>t3_hgknrq</td>\n",
              "      <td>Cycyvandemoosdijk</td>\n",
              "      <td>BeautyGuruChatter</td>\n",
              "      <td>Hell yeah !!!</td>\n",
              "      <td>1.593246e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>t1_fuwcfg9</td>\n",
              "      <td>25.0</td>\n",
              "      <td>t3_h9agyk</td>\n",
              "      <td>pinkglitterydolphins</td>\n",
              "      <td>BeautyGuruChatter</td>\n",
              "      <td>Things are pretty bad in the UK regarding the ...</td>\n",
              "      <td>1.592227e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>t1_gynjccr</td>\n",
              "      <td>10.0</td>\n",
              "      <td>t3_neuolg</td>\n",
              "      <td>transitionshade</td>\n",
              "      <td>BeautyGuruChatter</td>\n",
              "      <td>Right? It was their job to be sure before post...</td>\n",
              "      <td>1.621394e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>t1_fgsl4fw</td>\n",
              "      <td>8.0</td>\n",
              "      <td>t3_ezgt2t</td>\n",
              "      <td>WhisperInWater</td>\n",
              "      <td>BeautyGuruChatter</td>\n",
              "      <td>Honestly itâ€™s fine theyâ€™re not super pigmented...</td>\n",
              "      <td>1.581087e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>t1_gztf8ly</td>\n",
              "      <td>17.0</td>\n",
              "      <td>t3_nn1c4e</td>\n",
              "      <td>ediblesprysky</td>\n",
              "      <td>BeautyGuruChatter</td>\n",
              "      <td>I actually meant the yellow stuff on top ðŸ˜… I p...</td>\n",
              "      <td>1.622245e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>t1_gzik113</td>\n",
              "      <td>28.0</td>\n",
              "      <td>t3_nl29t6</td>\n",
              "      <td>thegigsup</td>\n",
              "      <td>BeautyGuruChatter</td>\n",
              "      <td>While driving, Iâ€™ve seen either their headquar...</td>\n",
              "      <td>1.622038e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>t1_fwi9064</td>\n",
              "      <td>9.0</td>\n",
              "      <td>t3_hiriqr</td>\n",
              "      <td>Mdawgydawg</td>\n",
              "      <td>BeautyGuruChatter</td>\n",
              "      <td>Thank you!! Was thinking the same thing, but c...</td>\n",
              "      <td>1.593545e+09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            id  score    link_id                author          subreddit  \\\n",
              "0   t1_fozlgkc   84.0  t3_ga8hsp             [deleted]  BeautyGuruChatter   \n",
              "1   t1_fw10hrc    2.0  t3_hf5j57        MGDlikethebeer  BeautyGuruChatter   \n",
              "2   t1_gijx7tq    6.0  t3_ksvuzc        uselesssubject  BeautyGuruChatter   \n",
              "3   t1_gi8bmyh    2.0  t3_kppctm             pitolaser  BeautyGuruChatter   \n",
              "4   t1_fofd5q7    6.0  t3_g6oxmu   forgotmyfuckingname  BeautyGuruChatter   \n",
              "5   t1_fvzbhby   22.0  t3_hf5j57           severaldogs  BeautyGuruChatter   \n",
              "6   t1_fvwg9vn    3.0  t3_hf5j57                 hygsi  BeautyGuruChatter   \n",
              "7   t1_fvpa78v   64.0  t3_he33fn           OneHappyOne  BeautyGuruChatter   \n",
              "8   t1_fw5dw20    4.0  t3_hgknrq     Cycyvandemoosdijk  BeautyGuruChatter   \n",
              "9   t1_fuwcfg9   25.0  t3_h9agyk  pinkglitterydolphins  BeautyGuruChatter   \n",
              "10  t1_gynjccr   10.0  t3_neuolg       transitionshade  BeautyGuruChatter   \n",
              "11  t1_fgsl4fw    8.0  t3_ezgt2t        WhisperInWater  BeautyGuruChatter   \n",
              "12  t1_gztf8ly   17.0  t3_nn1c4e         ediblesprysky  BeautyGuruChatter   \n",
              "13  t1_gzik113   28.0  t3_nl29t6             thegigsup  BeautyGuruChatter   \n",
              "14  t1_fwi9064    9.0  t3_hiriqr            Mdawgydawg  BeautyGuruChatter   \n",
              "\n",
              "                                                 body   created_utc  \n",
              "0   I felt like the British guy actually asked him...  1.588193e+09  \n",
              "1   Damn I missed it. I hope someone recorded it s...  1.593143e+09  \n",
              "2   Hmmm I was more wondering if it was affiliated...  1.610123e+09  \n",
              "3                        Thanks! Will check them out.  1.609884e+09  \n",
              "4   Same here. When I cook/bake recipes that take ...  1.587733e+09  \n",
              "5   I honestly think itâ€™s why he uploaded the demo...  1.593110e+09  \n",
              "6   I've noticed this too, there have been rumors ...  1.593039e+09  \n",
              "7   Yeah, I think that's what a lot of people get ...  1.592878e+09  \n",
              "8                                       Hell yeah !!!  1.593246e+09  \n",
              "9   Things are pretty bad in the UK regarding the ...  1.592227e+09  \n",
              "10  Right? It was their job to be sure before post...  1.621394e+09  \n",
              "11  Honestly itâ€™s fine theyâ€™re not super pigmented...  1.581087e+09  \n",
              "12  I actually meant the yellow stuff on top ðŸ˜… I p...  1.622245e+09  \n",
              "13  While driving, Iâ€™ve seen either their headquar...  1.622038e+09  \n",
              "14  Thank you!! Was thinking the same thing, but c...  1.593545e+09  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('sample.csv')\n",
        "df.head(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M0zfXn9WQBe"
      },
      "source": [
        "We find and remove \"deleted\" users or \"removed\" comments and any null values, and calculate the resulting number of rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "O8hec3pLRcwd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "41253"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Garbage count\n",
        "garbage_rows = (\n",
        "    df['author'].isin(['', '[deleted]']) | df['author'].isna() |\n",
        "    df['body'].isin(['', '[removed]']) | df['body'].isna()\n",
        ")\n",
        "garbage_count = df[garbage_rows].shape[0]\n",
        "garbage_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gmo808WLTN1G"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "407620"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Total count\n",
        "df.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vOwctRBbTR0I"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "366367"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Delete garbage rows from the DataFrame\n",
        "df = df[~garbage_rows]\n",
        "df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXrmQuxMX7Wv"
      },
      "source": [
        "We group by `author` and concatenate the body together to produce one aggregated string to be fed into the sentiment analyzer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vkqfny9qUMSz"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-----------_---</td>\n",
              "      <td>why red pickaxe honestly i think this is a jok...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>------__------------</td>\n",
              "      <td>I want computers to get twice as fast every 6 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-----iMartijn-----</td>\n",
              "      <td>A band is just a company and if you want it to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-----isaac-----</td>\n",
              "      <td>Thatâ€™s some of the best Star Wars lore, idk ab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-----username-----</td>\n",
              "      <td>Thatâ€™s what this is all about. The UK had to h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>----Oumeno----</td>\n",
              "      <td>PB pls make her an LI ill do anything ill pay ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>----Richard----</td>\n",
              "      <td>Yeah, I get you.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>----The_Truth-----</td>\n",
              "      <td>Your post was removed from r/PennyStocks becau...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>----yw----</td>\n",
              "      <td>i know what those are from, but why do they us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>---AL---</td>\n",
              "      <td>The Jack Daniels of churches.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 author                                               body\n",
              "0       -----------_---  why red pickaxe honestly i think this is a jok...\n",
              "1  ------__------------  I want computers to get twice as fast every 6 ...\n",
              "2    -----iMartijn-----  A band is just a company and if you want it to...\n",
              "3       -----isaac-----  Thatâ€™s some of the best Star Wars lore, idk ab...\n",
              "4    -----username-----  Thatâ€™s what this is all about. The UK had to h...\n",
              "5        ----Oumeno----  PB pls make her an LI ill do anything ill pay ...\n",
              "6       ----Richard----                                   Yeah, I get you.\n",
              "7    ----The_Truth-----  Your post was removed from r/PennyStocks becau...\n",
              "8            ----yw----  i know what those are from, but why do they us...\n",
              "9              ---AL---                      The Jack Daniels of churches."
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Group by 'author' and concatenate 'body' strings\n",
        "grouped_df = df.groupby('author')['body'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "grouped_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-QgmYKSYn_y"
      },
      "source": [
        "We create function that takes string as text and produces a sentiment number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iwwOKkdhYD5I"
      },
      "outputs": [],
      "source": [
        "def sentiment(text: str) -> float:\n",
        "  return sid.polarity_scores(text)['compound']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBtD7ll6ZPZ8"
      },
      "source": [
        "We apply the function to generate the sentiment score for a sample of 10 users, and print the entries out to check whether the generated scores makes sense for each text body. With the exception of `JakeFitzy7` and `proximateprose`, the sentiment score seems to make sense. The sentiment analyzer was able to get 8/10 right, which is better than average (at least for this sample). At first glance, it seems to work better for true positives compared to true negatives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "U5yMSw-aZs-H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Author: JakeFitzy7\n",
            "Body: I said *if he is* learn to read you autistic humunculous.\n",
            "Sentiment: 0.0\n",
            "\n",
            "Author: Revrun670\n",
            "Body: Yo who did this? ðŸ¤” Who did this to  Bobby ðŸ˜‚ðŸ˜‚ðŸ˜‚\n",
            "Sentiment: 0.0\n",
            "\n",
            "Author: LightningBoltZolt\n",
            "Body: I was thinking the same thing, especially with their track of keeping secrets that are plain to see. Thanks for your response!\n",
            "Sentiment: 0.4926\n",
            "\n",
            "Author: varjar\n",
            "Body: > goalrilla\n",
            "\n",
            "Was always jealous of the kids who had those.\n",
            "Sentiment: -0.4588\n",
            "\n",
            "Author: proximateprose\n",
            "Body: Guys who are usually \"decent people\" but will do terrible shit when given the opportunity can usually be dissuaded by letting them know you are gathering pieces of data in case they do anything to you (car make, model, license plate; first and last name; as many socials as you can get; etc.), but you do still have to **actually gather** that kind of info, which is really hard to care about and remember to do when manic, sorry.\n",
            "\n",
            "For the true shitbags, they're so used to deceiving women to get access that they've got their lies and schemes down pat; the above won't work. As a general proposition, steer clear of the **really** charming, sweet, etc. guys; them's your predators. Hotel/motel room is safer, if only because it makes it harder to stalk you later or kill you now if they are so inclined. \n",
            "\n",
            "Also maybe bring hairspray & a lighter and a big (but not so big you can't use it) knife. Better safe than sorry.\n",
            "\n",
            "Obviously I'd strongly recommend investing in a variety of sex toys and/or discovering a heretofore unknown attraction to folx other than men as opposed to sleeping with random guys, but, I get it: mania's gonna mania.\n",
            "Sentiment: 0.965\n",
            "\n",
            "Author: dniv\n",
            "Body: Thank you, I appreciate that. And Iâ€™m jelly, ability to play without spreadsheets is impressive. I can to an extent but itâ€™s not worth it since I want the ultimate experience. Iâ€™d say they were deuteragonists. But yes I totally agree with how Falcom should have more female leads even if theyâ€™re co-leads. They tend to write them very well. Youâ€™d put 6 above 9? Iâ€™ve only played part of 6 so canâ€™t really judge it, but was just curious.\n",
            "Sentiment: 0.9579\n",
            "\n",
            "Author: blueknight1758\n",
            "Body: How do you get the revolution in the new update?\n",
            "Sentiment: 0.0\n",
            "\n",
            "Author: peepeepoopoo999999\n",
            "Body: not just russians, pretty much any pro authoritarian or pro fascist members of any country. although russians seem to have the monopoly on that\n",
            "Sentiment: -0.7374\n",
            "\n",
            "Author: NoLongerAGame\n",
            "Body: Fair enough and makes sense as that's the only way you can manage both jp and global and Genshin while having a life. I personally stopped trying to keep up with 7ds ages ago when I realised the breakneck speed they were going with content. I do one hell demon sometimes not everyday and on weekends I just do 3 extreme. Can't be bothered.\n",
            "Sentiment: -0.3481\n",
            "\n",
            "Author: Mob_King\n",
            "Body: What a brilliant concept and execution for an interview. Well done and would love to see more stuff like this.\n",
            "Sentiment: 0.9166\n",
            "\n"
          ]
        }
      ],
      "source": [
        "demo_sample = grouped_df.sample(n=10, random_state=42)\n",
        "demo_sample['sentiment'] = demo_sample['body'].apply(sentiment)\n",
        "\n",
        "# Display the sampled DataFrame with sentiment\n",
        "for index, row in demo_sample.iterrows():\n",
        "    print(f\"Author: {row['author']}\\nBody: {row['body']}\\nSentiment: {row['sentiment']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwBXotK1fLWY"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
