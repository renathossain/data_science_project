{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZHvH-6y51J3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn as sk\n",
        "import nltk\n",
        "import random\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "# Download the VADER lexicon for sentiment analysis\n",
        "nltk.download('vader_lexicon')\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "data_path = \"text_comments.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first explore the data in `text_comments.csv` by printing some rows."
      ],
      "metadata": {
        "id": "L6Yf7FU7U4BR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create chunker\n",
        "chunk_size = 40000\n",
        "tf_chunks = pd.read_csv(data_path, chunksize=chunk_size)\n",
        "\n",
        "# Display the first 10 rows\n",
        "print(tf_chunks.get_chunk(10))"
      ],
      "metadata": {
        "id": "4BoA6m5s74aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We explore how many comments a subreddit could have."
      ],
      "metadata": {
        "id": "MFML2jIWVCG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_chunk = next(tf_chunks)\n",
        "first_chunk[\"subreddit\"].value_counts(dropna=False)"
      ],
      "metadata": {
        "id": "2F23Wi418AXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we introduce the sentiment analysis tool called VADER (Valence Aware Dictionary and sEntiment Reasoner). The sentiment scores are represented as a dictionary with the following keys:\n",
        "\n",
        "    'neg': Negative sentiment score (proportion of the text that is negative)\n",
        "    'neu': Neutral sentiment score (proportion of the text that is neutral)\n",
        "    'pos': Positive sentiment score (proportion of the text that is positive)\n",
        "    'compound': Compound sentiment score (a normalized, weighted composite score that represents the overall sentiment of the text)\n"
      ],
      "metadata": {
        "id": "Q_Nr6clkBXkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text (replace this with your own data)\n",
        "text = \"I really enjoyed working with my team. They are so helpful and supportive.\"\n",
        "\n",
        "# Initialize the sentiment analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Get the sentiment scores\n",
        "sentiment_scores = sid.polarity_scores(text)\n",
        "\n",
        "# Display the sentiment scores\n",
        "print(\"Sentiment Scores:\", sentiment_scores)"
      ],
      "metadata": {
        "id": "XFN4MVWz8iU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we take a sample of the dataset 100 to 1. (Do not run this, we have made the sample already)"
      ],
      "metadata": {
        "id": "NtED7ajGVuhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the number of rows to read in each chunk\n",
        "chunk_size = 1000\n",
        "\n",
        "# Specify the number of rows to sample from each chunk\n",
        "sample_size = 10\n",
        "\n",
        "# Specify the path for the output CSV file\n",
        "output_csv_path = 'sample.csv'\n",
        "\n",
        "# Create a CSV writer object for the first chunk\n",
        "first_chunk = True\n",
        "\n",
        "# Create a CSV reader object\n",
        "csv_reader = pd.read_csv(data_path, chunksize=chunk_size, encoding='utf-8')\n",
        "\n",
        "# Iterate over each chunk, sample 10 rows, and append them to the output CSV file\n",
        "for i, chunk in enumerate(csv_reader):\n",
        "    try:\n",
        "      # Sample 10 rows from each chunk\n",
        "      sampled_chunk = chunk.sample(n=sample_size, random_state=42)  # Adjust random_state as needed\n",
        "\n",
        "      # Append the sampled chunk to the output CSV file\n",
        "      sampled_chunk.to_csv(output_csv_path, mode='a', index=False, header=first_chunk)\n",
        "\n",
        "      # Update the first_chunk flag after the first iteration\n",
        "      first_chunk = False\n",
        "    except:\n",
        "      print(f\"Error in chunk {i + 1}: {e}\")"
      ],
      "metadata": {
        "id": "Pd9-5ME-_xR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We explore the sample dataset."
      ],
      "metadata": {
        "id": "Mql5YFZjWHbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('sample.csv')\n",
        "df.head(15)"
      ],
      "metadata": {
        "id": "nKRl34m2JLA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We find and remove \"deleted\" users or \"removed\" comments and any null values, and calculate the resulting number of rows."
      ],
      "metadata": {
        "id": "9M0zfXn9WQBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Garbage count\n",
        "garbage_rows = (\n",
        "    df['author'].isin(['', '[deleted]']) | df['author'].isna() |\n",
        "    df['body'].isin(['', '[removed]']) | df['body'].isna()\n",
        ")\n",
        "garbage_count = df[garbage_rows].shape[0]\n",
        "garbage_count"
      ],
      "metadata": {
        "id": "O8hec3pLRcwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total count\n",
        "df.shape[0]"
      ],
      "metadata": {
        "id": "gmo808WLTN1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete garbage rows from the DataFrame\n",
        "df = df[~garbage_rows]\n",
        "df.shape[0]"
      ],
      "metadata": {
        "id": "vOwctRBbTR0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We group by `author` and concatenate the body together to produce one aggregated string to be fed into the sentiment analyzer."
      ],
      "metadata": {
        "id": "WXrmQuxMX7Wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by 'author' and concatenate 'body' strings\n",
        "grouped_df = df.groupby('author')['body'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "grouped_df.head(10)"
      ],
      "metadata": {
        "id": "vkqfny9qUMSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create function that takes string as text and produces a sentiment number."
      ],
      "metadata": {
        "id": "u-QgmYKSYn_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment(text: str) -> float:\n",
        "  return sid.polarity_scores(text)['compound']"
      ],
      "metadata": {
        "id": "iwwOKkdhYD5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply the function to generate the sentiment score for a sample of 10 users, and print the entries out to check whether the generated scores makes sense for each text body. With the exception of `JakeFitzy7` and `proximateprose`, the sentiment score seems to make sense. The sentiment analyzer was able to get 8/10 right, which is better than average (at least for this sample). At first glance, it seems to work better for true positives compared to true negatives."
      ],
      "metadata": {
        "id": "QBtD7ll6ZPZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo_sample = grouped_df.sample(n=10, random_state=42)\n",
        "demo_sample['sentiment'] = demo_sample['body'].apply(sentiment)\n",
        "\n",
        "# Display the sampled DataFrame with sentiment\n",
        "for index, row in demo_sample.iterrows():\n",
        "    print(f\"Author: {row['author']}\\nBody: {row['body']}\\nSentiment: {row['sentiment']}\\n\")"
      ],
      "metadata": {
        "id": "U5yMSw-aZs-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GS **Part**"
      ],
      "metadata": {
        "id": "HwBXotK1fLWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Bigger Random Sample to train community embeddings\n",
        "full_size = len(df)*100\n",
        "train_size = 400000\n",
        "meta_cols = [3,4,6]\n",
        "\n",
        "print(full_size)\n",
        "\n",
        "np.random.seed(0) # fix seed\n",
        "to_skip = np.arange(1, full_size)\n",
        "np.random.shuffle(to_skip)\n",
        "to_skip = to_skip[:full_size-train_size+1]\n",
        "to_skip\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WyfsdrFZHcrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(data_path, skiprows=to_skip, usecols=meta_cols)\n"
      ],
      "metadata": {
        "id": "bb8gMY-FQ-BO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identify unique authors and subreddits, and observe distribution of posts"
      ],
      "metadata": {
        "id": "YdKVZoAtBtnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#print(df)\n",
        "# add symbols to user names to make them distinguishable from subreddits in embedding\n",
        "sign = '/'\n",
        "\n",
        "df.author = df.author.apply(lambda a: sign + a)\n",
        "\n",
        "\n",
        "uniqueAuthors = df.groupby('author').agg({'score': 'size'}).rename(columns={'score': 'count'})\n",
        "uniqueReddits = df.groupby('subreddit').agg({'score': 'size'}).rename(columns={'score': 'count'})\n",
        "\n",
        "#train_uniqueAuthors = train_df.groupby('author').agg({'score': 'size'}).rename(columns={'score': 'count'})\n",
        "#train_uniqueReddits = train_df.groupby('subreddit').agg({'score': 'size'}).rename(columns={'score': 'count'})"
      ],
      "metadata": {
        "id": "t-Q1bmuifubc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(df.head(5))\n",
        "\n",
        "plt.loglog(uniqueAuthors[:200].sort_values(by='count', ascending=False))\n",
        "plt.title('Author Contributions')\n",
        "plt.ylabel('number of contributions')\n",
        "plt.xlabel('author')\n",
        "plt.xticks(rotation=90)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.loglog(uniqueReddits[:200].sort_values(by='count', ascending=False))\n",
        "plt.title('Subreddit Activity')\n",
        "plt.ylabel('number of contributions')\n",
        "plt.xlabel('subreddit')\n",
        "plt.xticks(rotation=90)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "YElxFKlNS87F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Pairs of D"
      ],
      "metadata": {
        "id": "BAP51DiNCDPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to generate (ci, uj) for every user j who commented in community ci\n",
        "communityUsers = df.groupby(by=['subreddit', 'author']).agg({'score':'size', 'created_utc':'min'})\n",
        "communityUsers.head(5)"
      ],
      "metadata": {
        "id": "2But8XQLHfTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tuples = communityUsers.index\n",
        "print(train_tuples)\n",
        "\n",
        "alpha = .18\n",
        "size = 150"
      ],
      "metadata": {
        "id": "ET1bt9r2IOC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(train_tuples, alpha=alpha, vector_size=size)\n",
        "model.train(train_tuples, total_examples=len(train_tuples), epochs=2)"
      ],
      "metadata": {
        "id": "knwm8WVEIY1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Community Embeddings into DataFrame\n",
        "wvs = model.wv.get_normed_vectors()\n",
        "keys = model.wv.key_to_index.keys()\n",
        "\n",
        "communityEmbeddings = pd.DataFrame(wvs, index=keys)\n",
        "\n",
        "communityEmbeddings.drop(index=communityEmbeddings[communityEmbeddings.index.str.startswith(sign)].index, inplace=True)\n",
        "communityEmbeddings.tail(10)\n",
        "print(communityEmbeddings)"
      ],
      "metadata": {
        "id": "yufdkN47Idez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "wv_pca = PCA(n_components=3).fit_transform(communityEmbeddings)\n",
        "pca_df = pd.DataFrame(wv_pca, index=communityEmbeddings.index, columns=['x','y','z'])\n",
        "print(pca_df)\n",
        "\n",
        "import plotly.express as px\n",
        "fig = px.scatter_3d(pca_df[:200], x='x', y='y', z='z', text=communityEmbeddings.index[:200])\n",
        "fig.show(renderer='colab')\n"
      ],
      "metadata": {
        "id": "FW3rbYUCI2Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate COM for all users\n",
        "uniqueAuthors['com'] = 0\n",
        "\n",
        "print(len(uniqueReddits))\n",
        "print(uniqueReddits.loc['SaltLakeCity'])\n",
        "#print('SaltLakeCity' in keys)\n",
        "\n",
        "\"\"\"\n",
        "# Use this when dataset is absurdly large to track progress\n",
        "batch_size = 1000\n",
        "for start, end in zip(range(0,len(df)-batch_size, batch_size),\n",
        "                      range(batch_size+1, len(df),\n",
        "                            batch_size)):\n",
        "  temp = df.iloc[start:end].groupby('author').agg(\n",
        "      {'subreddit': lambda srs: communityEmbeddings.loc[srs].mean().to_list()})\n",
        "  print(temp)\n",
        "  uniqueAuthors.loc[temp.index, 'com'] = temp.subreddit\n",
        "  print('start:', start)\n",
        "\"\"\"\n",
        "print(communityEmbeddings.index)\n",
        "\n",
        "#communityEmbeddings.loc['Kikpals'] #, 'bostonr4r', 'Kikpals', 'feckingbirds']]\n",
        "# Filter out subreddits not in embedding\n",
        "\n",
        "df1 = df[df.subreddit.isin(communityEmbeddings.index)]\n",
        "print(df1)\n",
        "\n",
        "temp = df1.groupby('author').agg({'subreddit':\n",
        "        lambda srs: communityEmbeddings.loc[srs].mean().to_list()})\n",
        "uniqueAuthors.loc[temp.index, 'com'] = temp.subreddit\n",
        "\n",
        "print(uniqueAuthors)"
      ],
      "metadata": {
        "id": "FhnkMNqNJFNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate GS Score\n",
        "\n",
        "\n",
        "#print(sample)\n",
        "#print(uniqueAuthors.loc[sample.index.get_level_values('author'), 'com'], communityEmbeddings.loc[sample.index.get_level_values('subreddit')])\n",
        "#sample['dot'] = np.dot(uniqueAuthors.loc[sample.index.get_level_values('author'), 'com'],\n",
        " #                      communityEmbeddings.loc[sample.index.get_level_values('subreddit')])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Omit authors for whom com was not computed\n",
        "com_computed = uniqueAuthors.com != 0\n",
        "curatedAuthors = uniqueAuthors.loc[com_computed]\n",
        "\n",
        "# compute dot products\n",
        "def get_gs(srs):\n",
        "  gs = srs.apply(lambda s:\n",
        "                  np.dot(communityEmbeddings.loc[s, :].array,\n",
        "                         curatedAuthors.loc[df1.loc[srs.index[0]].author, 'com'])).sum()\n",
        "\n",
        "  gs = gs / srs.size\n",
        "  norm = np.linalg.norm(curatedAuthors.loc[df1.loc[srs.index[0]].author, 'com'])\n",
        "  gs = gs / norm\n",
        "\n",
        "\n",
        "  return gs\n",
        "\n",
        "uniqueAuthors['gs'] = -1\n",
        "gs = df1.groupby('author').agg({'subreddit': get_gs}).subreddit\n",
        "print(gs)\n",
        "\n",
        "gs.plot.hist(bins=20)\n",
        "\n",
        "\"\"\"\n",
        "# This is a partial calculation (we still need to divide by com norm and )\n",
        "temp = df1.groupby('author').agg(\n",
        "    {'subreddit':\n",
        "       lambda srs: (1. / srs.nunique()) \\\n",
        "       * srs.apply(lambda v: np.dot(v, uniqueAuthors[df1.loc[srs.index].author)) \\\n",
        "        })\n",
        "uniqueAuthors.loc[temp.index, 'gs'] = temp.subreddit\n",
        "\n",
        "\n",
        "communityEmbeddings.loc[df1.loc[df1.author == x.index].subreddit].apply(\n",
        "        lambda v:\n",
        "        np.dot(v, uniqueAuthors.loc[x.index].com)\"\"\"\n",
        "\n",
        "\n",
        "\"\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6NV7VsIWIpoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(x=gs, bins=np.linspace(0, 2, 50))"
      ],
      "metadata": {
        "id": "YgC7hNNI6smi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pU6EyXKJ6ukg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uniqueAuthors.gs = gs"
      ],
      "metadata": {
        "id": "xy01lfNDzIzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.linalg.norm?\n",
        "print(curatedAuthors.com.values)\n",
        "\n",
        "norms = curatedAuthors.com.apply(lambda v: np.linalg.norm(v))\n"
      ],
      "metadata": {
        "id": "R_vwqJ3Kvaew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(temp, norms)\n",
        "\n",
        "gs = temp / norms\n",
        "#gs /= df1.groupby('author').agg({'subreddit': 'nunique'}).subreddit\n",
        "\n",
        "print(gs)\n",
        "\n",
        "ads = df1.groupby('author').agg('text').apply(sentiment)"
      ],
      "metadata": {
        "id": "9Ma-2SyK1eDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uniqueAuthors.ad = ads\n",
        "\n",
        "plt.scatter(uniqueAuthors['ad'], uniqueAuthors['gs'])\n",
        "plt.xlabel('ad')\n",
        "plt.ylabel('gs')\n",
        "plt.title('Scatter Plot of ad vs gs')\n",
        "plt.show()\n",
        "\n",
        "correlation = uniqueAuthors['ad'].corr(uniqueAuthors['gs'])\n",
        "\n",
        "print(f\"Correlation between 'ad' and 'gs': {correlation}\")"
      ],
      "metadata": {
        "id": "8LTWcup6gseo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}